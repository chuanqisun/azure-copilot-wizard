// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
export const parser = LRParser.deserialize({
  version: 14,
  states: "#SOQOPOOOVOPO'#C_O[OPO'#C^OOOO'#Ce'#CeQQOPOOOaOPO,58yOiOQO,58xOOOO-E6c-E6cOzOPO'#CfO!POPO1G.eOOOO'#Cb'#CbOOOO'#Cg'#CgO!XOQO1G.dOOOO,59Q,59QOOOO-E6d-E6dOOOO-E6e-E6e",
  stateData: "!j~O]PO~OSTO~OTUO~O]WOTRa~OTYOVYOWYO[Qa]Qa~OS]O~O]WOTRi~OTYOVYOWYO[Qi]Qi~O",
  goto: "{[PP]aPPePPiouTROSTQOSTZU[QSORVSQXTR^XQ[UR_[",
  nodeNames: "âš  File Statement Path Segment Space Text Number Char",
  maxTerm: 13,
  skippedNodes: [0],
  repeatNodeCount: 3,
  tokenData: "#r~R_X^!Qpq!Q!P!Q!u!Q![!z!c!}#S#R#S#d#T#o#S#y#z!Q$f$g!Q#BY#BZ!Q$IS$I_!Q$I|$JO!Q$JT$JU!Q$KV$KW!Q&FU&FV!Q~!VYT~X^!Qpq!Q#y#z!Q$f$g!Q#BY#BZ!Q$IS$I_!Q$I|$JO!Q$JT$JU!Q$KV$KW!Q&FU&FV!Q~!zO]~~#PPV~!Q![!zR#ZRWQSP!c!}#S#R#S#d#T#o#SP#iRSP!c!}#d#R#S#d#T#o#d",
  tokenizers: [0, 1],
  topRules: {"File":[0,1]},
  tokenPrec: 0
})

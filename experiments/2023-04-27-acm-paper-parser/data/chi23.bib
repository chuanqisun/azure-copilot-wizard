@inproceedings{10.1145/3544548.3581112,
author = {Chen, Yingting and Kanno, Taro and Furuta, Kazuo},
title = {Cognition-Oriented Facilitation and Guidelines for Collaborative Problem-Solving Online and Face-to-Face: An in-Depth Examination of Format and Facilitation Influence on Problem-Solving Performance},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581112},
doi = {10.1145/3544548.3581112},
abstract = {During the Covid-19 pandemic, more guidelines were created to teach people how to facilitate meetings online, but few were designed from a cognition-oriented perspective. Additionally, solving complex problems is essential in many occupations. However, the influence of online and face-to-face discussion formats on the performance in complex problem-solving tasks is unclear, even though remote working has become common over the past several few years. Hence, this study aims to answer two research questions: (a) Does problem-solving performance differ between online and face-to-face meetings? and (b) Does facilitation improve problem-solving performance when different formats are used? We conducted experiments with 40 groups using a 2 \texttimes{} 2 factorial design, which were controlled for both facilitation and format. Each group comprised two randomly selected participants, and each problem-solving discussion lasted between 1.5–2 h. The obtained evidence showed that format can influence the performance of balancing intercorrelated factors in a complex scenario, but it does not affect the performance of achieving a predefined goal. Instead, it we found that facilitation is helpful for achieving a predefined goal. Based on the results obtained, we propose future design directions for problem-solving centric computer-supported cooperative work systems from a cognition-oriented perspective.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {61},
numpages = {15},
keywords = {Meeting facilitation, Face-to-face meeting, Complex problem solving, Cognition-oriented guidelines, Remote collaboration},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581556,
author = {Williams, Rua Mae and Park, Chorong},
title = {Cyborg Assemblages: How Autistic Adults Construct Sociotechnical Networks to Support Cognitive Function},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581556},
doi = {10.1145/3544548.3581556},
abstract = {Autism has become a popular context for accessible technology researchers, yet a majority of HCI projects for autism and ADHD do not engage in participatory methods or otherwise involve disabled stakeholders in the project and research design. Prior inquiry has identified executive function as a common difficulty for which technologies may provide novel benefits. In this study, we explore how autistic adults currently use technologies, broadly defined, to augment executive function and support themselves in day-to-day tasks. We collect qualitative data from narratives elicited during informal asynchronous interviews to conduct a digital ethnomethodology. Following from principles of Design Justice, crip technoscience, and cyborg assemblage theory, we investigate how autistic adults articulate their own sociotechnical environments into technologically mediated assemblages of executive function and interpersonal webs of care. These patterns of sociotechnical formation inform future work in research and design for tools that can mediate executive function for all users.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {62},
numpages = {15},
keywords = {critical disability studies, executive function, contextual inquiry, digital ethnomethodology, disability in adulthood},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580824,
author = {Huber, Stephan and Rath\ss{}, Natalie},
title = {Empathic Accuracy and Mental Effort During Remote Assessments of Emotions},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580824},
doi = {10.1145/3544548.3580824},
abstract = {Observing users in remote settings is unfavorable because it adds filters altering the information that underlie judgement. Still, the COVID pandemic led to an unprecedented popularity of remote user experience tests. In this work, we revisited the question, which information is most important for evaluators to assess users’ emotions successfully and efficiently. In an online study, we asked N=55 participants to assess users’ emotions from short videos of 30 interaction situations. As independent variable, we manipulated the combination of the information channels video of users, video of the interactive technology, and audio within subjects. Our findings indicate that empathic accuracy is highest and mental effort is lowest when all stimuli are present. Surprisingly, empathic accuracy was lowest and mental effort highest, when only video of users was available. We discuss these findings in the light of emotion literature focusing on persons’ facial expressions and derive practical implications for remote observations.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {63},
numpages = {13},
keywords = {meta-evaluation, Remote user research, emotion, empathy, observations},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580742,
author = {Son, Seoyun and Choi, Junyoug and Lee, Sunjae and Song, Jean Y and Shin, Insik},
title = {It is Okay to Be Distracted: How Real-Time Transcriptions Facilitate Online Meeting with Distraction},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580742},
doi = {10.1145/3544548.3580742},
abstract = {Online meetings are indispensable in collaborative remote work environments, but they are vulnerable to distractions due to their distributed and location-agnostic nature. While distraction often leads to a decrease in online meeting quality due to loss of engagement and context, natural multitasking has positive tradeoff effects, such as increased productivity within a given time unit. In this study, we investigate the impact of real-time transcriptions (i.e., full-transcripts, summaries, and keywords) as a solution to help facilitate online meetings during distracting moments while still preserving multitasking behaviors. Through two rounds of controlled user studies, we qualitatively and quantitatively show that people can better catch up with the meeting flow and feel less interfered with when using real-time transcriptions. The benefits of real-time transcriptions were more pronounced after distracting activities. Furthermore, we reveal additional impacts of real-time transcriptions (e.g., supporting recalling contents) and suggest design implications for future online meeting platforms where these could be adaptively provided to users with different purposes.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {64},
numpages = {19},
keywords = {multitasking, Video-conferencing, online meeting, real-time transcriptions, distraction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581485,
author = {Villa, Steeven and Niess, Jasmin and Nakao, Takuro and Lazar, Jonathan and Schmidt, Albrecht and Machulla, Tonja-Katrin},
title = {Understanding Perception of Human Augmentation: A Mixed-Method Study},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581485},
doi = {10.1145/3544548.3581485},
abstract = {Technologies that help users overcome their limitations and integrate with the human body are often termed “human augmentations”. Such technologies are now available on the consumer market, potentially supporting people in their everyday activities. To date, there is no systematic understanding of the perception of human augmentations yet. To address this gap and build an understanding of how to design positive experiences with human augmentations, we conducted a mixed-method study of the perception of augmented humans (AHs). We conducted two scenario-based studies: interviews (n = 16) and an online study (n = 506) with participants from four countries. The scenarios include one out of three augmentation categories (sensory, motor, and cognitive) and specify if the augmented person has a disability or not. Overall, results show that the type of augmentation and disability impacted user attitudes towards AHs. We derive design dimensions for creating technological augmentations for a diverse and global audience.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {65},
numpages = {16},
keywords = {social attitudes, augmented human, human augmentation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581355,
author = {Lee, Ken Jen and Davila, Adrian and Cheng, Hanlin and Goh, Joslin and Nilsen, Elizabeth and Law, Edith},
title = {“We Need to Do More... I Need to Do More”: Augmenting Digital Media Consumption via Critical Reflection to Increase Compassion and Promote Prosocial Attitudes and Behaviors},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581355},
doi = {10.1145/3544548.3581355},
abstract = {Much HCI research on prompting prosocial behaviors focuses on methods for increasing empathy. However, increased empathy may have unintended negative consequences. Our work offers an alternative solution that encourages critical reflection for nurturing compassion, which involves motivation and action to help others. In a between-subject experiment, participants (N=60) viewed a climate change documentary while receiving no prompts (CON), reflective prompts to focus on their emotions (RE) or surprises (RS). State compassion, critical reflection, and motivation to act or learn were measured at the end of the session (post-video) and two weeks later (follow-up). Despite participants’ condition not affecting compassion, critical reflection was positively correlated with post-video state compassion. RE and RS participants demonstrated deeper reflection and reported higher motivation to learn post-video, and more prosocial behavioral changes during follow-up. RS participants reported better follow-up recall than RE participants. We conclude by discussing implications on designing technology to support compassion and longer-term critical reflection.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {66},
numpages = {20},
keywords = {Prosocial Behaviors, Prosocial Attitudes, Digital Media, Critical Reflection, Compassion},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580683,
author = {Mcnutt, Andrew M and Outkine, Anton and Chugh, Ravi},
title = {A Study of Editor Features in a Creative Coding Classroom},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580683},
doi = {10.1145/3544548.3580683},
abstract = {Creative coding is a rapidly expanding domain for both artistic expression and computational education. Numerous libraries and IDEs support creative coding, however there has been little consideration of how the environments themselves might be designed to serve these twin goals. To investigate this gap, we implemented and used an experimental editor to teach a sequence of college and high-school creative coding courses. In the first year, we conducted a log analysis of student work (n=39) and surveys regarding prospective features (n=25). These guided our implementation of common enhancements (e.g. color pickers) as well as uncommon ones (e.g. bidirectional shape editing). In the second year, we studied the effects of these features through logging (n=39+) and survey (n=23) studies. Reflecting on the results, we identify opportunities to improve creativity- and novice-focused IDEs and highlight tensions in their design—as in tools that augment artistry or efficiency but may be perceived as hindering learning.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {121},
numpages = {15},
keywords = {Code editors, Introductory programming, p5, Creative coding},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581070,
author = {Shi, Xinyu and Zhou, Ziqi and Zhang, Jing Wen and Neshati, Ali and Tyagi, Anjul Kumar and Rossi, Ryan and Guo, Shunan and Du, Fan and Zhao, Jian},
title = {De-Stijl: Facilitating Graphics Design with Interactive 2D Color Palette Recommendation},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581070},
doi = {10.1145/3544548.3581070},
abstract = {Selecting a proper color palette is critical in crafting a high-quality graphic design to gain visibility and communicate ideas effectively. To facilitate this process, we propose De-Stijl, an intelligent and interactive color authoring tool to assist novice designers in crafting harmonic color palettes, achieving quick design iterations, and fulfilling design constraints. Through De-Stijl, we contribute a novel 2D color palette concept that allows users to intuitively perceive color designs in context with their proportions and proximities. Further, De-Stijl implements a holistic color authoring system that supports 2D palette extraction, theme-aware and spatial-sensitive color recommendation, and automatic graphical elements (re)colorization. We evaluated De-Stijl through an in-lab user study by comparing the system with existing industry standard tools, followed by in-depth user interviews. Quantitative and qualitative results demonstrate that De-Stijl is effective in assisting novice design practitioners to quickly colorize graphic designs and easily deliver several alternatives.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {122},
numpages = {19},
keywords = {2D color palette., Graphics design, AI-assisted design, color palette recommendation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580642,
author = {Hashim, Sonia and H\"{o}llerer, Tobias and Jacobs, Jennifer},
title = {Drawing Transforms: A Unifying Interaction Primitive to Procedurally Manipulate Graphics across Style, Space, and Time},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580642},
doi = {10.1145/3544548.3580642},
abstract = {Procedural functionality enables visual creators to rapidly edit, explore alternatives, and fine-tune artwork in many domains including illustration, motion graphics, and interactive animation. Symbolic procedural tools, such as textual programming languages, are highly expressive but often limit directly manipulating concrete artwork; whereas direct manipulation tools support some procedural expression but limit creators to pre-defined behaviors and inputs. Inspired by visions of using geometric input to create procedural relationships, we identify an opportunity to use vector geometry from artwork to specify expressive user-defined procedural functions. We present Drawing Transforms (DTs), a technique that enables the use of any drawing to procedurally transform the stylistic, spatial, and temporal properties of target artwork. We apply DTs in a prototype motion graphics system to author continuous and discrete transformations, modify multiple elements in a composition simultaneously, create animations, and control fine-grained procedural instantiation. We discuss how DTs can unify procedural authoring through direct manipulation across visual media domains.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {123},
numpages = {15},
keywords = {direct manipulation, creativity support tools, procedural art},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580931,
author = {Kato, Jun and Goto, Masataka},
title = {Lyric App Framework: A Web-Based Framework for Developing Interactive Lyric-Driven Musical Applications},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580931},
doi = {10.1145/3544548.3580931},
abstract = {Lyric videos have become a popular medium to convey lyrical content to listeners, but they present the same content whenever they are played and cannot adapt to listeners’ preferences. Lyric apps, as we name them, are a new form of lyric-driven visual art that can render different lyrical content depending on user interaction and address the limitations of static media. To open up this novel design space for programmers and musicians, we present Lyric App Framework, a web-based framework for building interactive graphical applications that play musical pieces and show lyrics synchronized with playback. We designed the framework to provide a streamlined development experience for building production-ready lyric apps with creative coding libraries of choice. We held programming contests twice and collected 52 examples of lyric apps, enabling us to reveal eight representative categories, confirm the framework’s effectiveness, and report lessons learned.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {124},
numpages = {18},
keywords = {multimedia control, toolkit, music synchronization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581338,
author = {Du, Ruofei and Li, Na and Jin, Jing and Carney, Michelle and Miles, Scott and Kleiner, Maria and Yuan, Xiuxiu and Zhang, Yinda and Kulkarni, Anuva and Liu, Xingyu and Sabie, Ahmed and Orts-Escolano, Sergio and Kar, Abhishek and Yu, Ping and Iyengar, Ram and Kowdle, Adarsh and Olwal, Alex},
title = {Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581338},
doi = {10.1145/3544548.3581338},
abstract = {In recent years, there has been a proliferation of multimedia applications that leverage machine learning (ML) for interactive experiences. Prototyping ML-based applications is, however, still challenging, given complex workflows that are not ideal for design and experimentation. To better understand these challenges, we conducted a formative study with seven ML practitioners to gather insights about common ML evaluation workflows. The study helped us derive six design goals, which informed Rapsai1, a visual programming platform for rapid and iterative development of end-to-end ML-based multimedia applications. Rapsai features a node-graph editor to facilitate interactive characterization and visualization of ML model performance. Rapsai streamlines end-to-end prototyping with interactive data augmentation and model comparison capabilities in its no-coding environment. Our evaluation of Rapsai in four real-world case studies (N=15) suggests that practitioners can accelerate their workflow, make more informed decisions, analyze strengths and weaknesses, and holistically evaluate model behavior with real-world input.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {125},
numpages = {23},
keywords = {Visual Analytics, Node-graph Editor, Model Comparison, Deep Neural Networks, Deep Learning, Data Augmentation, Visual Programming},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581394,
author = {Rawn, Eric and Li, Jingyi and Paulos, Eric and Chasins, Sarah E.},
title = {Understanding Version Control as Material Interaction with Quickpose},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581394},
doi = {10.1145/3544548.3581394},
abstract = {Whether a programmer with code or a potter with clay, practitioners engage in an ongoing process of working and reasoning with materials. Existing discussions in HCI have provided rich accounts of these practices and processes, which we synthesize into three themes: (1) reciprocal discovery of goals and materials, (2) local knowledge of materials, and (3) annotation for holistic interpretation. We then apply these design principles generatively to the domain of version control to present Quickpose: a version control system for creative coding. In an in-situ, longitudinal study of Quickpose guided by our themes, we collected usage data, version history, and interviews. Our study explored our participants’ material interaction behaviors and the initial promise of our proposed measures for recognizing these behaviors. Quickpose is an exploration of version control as material interaction, using existing discussions to inform domain-specific concepts, measures, and designs for version control systems.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {126},
numpages = {18},
keywords = {Materiality, Variations, Creative Coding, End-User Programming, Version Control Systems (VCS)},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581149,
author = {Sultana, Sharifa and Ahmed, Syed Ishtiaque and Rzeszotarski, Jeffrey M},
title = {Communicating Consequences: Visual Narratives, Abstraction, and Polysemy in Rural Bangladesh},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581149},
doi = {10.1145/3544548.3581149},
abstract = {Information communication and visualization practices reflect two centuries of developments of conventions and best practices which may not be reflective of global audiences’ methods for conveying information. Contrasting between rural traditional visual culture and contemporary HCI and data-visualization, we argue that an understanding of traditional practices for information visualization is required for building rich data-narratives and making data-driven systems more accessible and culturally situated. Our ten-month ethnographic study investigates how rural Bangladeshi communities construct narratives through visual media. 1 Our observation, interviews, and FGDs (n=54) expose how participants convey risk management, decision-making, and monetary management practices to their peers. We find that villagers used a rich network of polysemic symbols and abstractions to manifest subjectivity, factuality, consequence, situatedness, and uncertainty; varied visual attributes for constructing narratives; and emphasized material relations among components in visuals. These findings inform the design of future systems for decision support in a culturally situated manner.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {169},
numpages = {19},
keywords = {Bangladesh, Visual Narratives, Abstraction, Polysemy, Information Visualization, Consequences},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581470,
author = {Li, Wenchao and Wang, Zhan and Wang, Yun and Weng, Di and Xie, Liwenhan and Chen, Siming and Zhang, Haidong and Qu, Huamin},
title = {GeoCamera: Telling Stories in Geographic Visualizations with Camera Movements},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581470},
doi = {10.1145/3544548.3581470},
abstract = {In geographic data videos, camera movements are frequently used and combined to present information from multiple perspectives. However, creating and editing camera movements requires significant time and professional skills. This work aims to lower the barrier of crafting diverse camera movements for geographic data videos. First, we analyze a corpus of 66 geographic data videos and derive a design space of camera movements with a dimension for geospatial targets and one for narrative purposes. Based on the design space, we propose a set of adaptive camera shots and further develop an interactive tool called GeoCamera. This interactive tool allows users to flexibly design camera movements for geographic visualizations. We verify the expressiveness of our tool through case studies and evaluate its usability with a user study. The participants find that the tool facilitates the design of camera movements.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {170},
numpages = {15},
keywords = {geographic visualization, data video, authoring tools, Visual storytelling},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580701,
author = {Xu, Xian and Wu, Aoyu and Yang, Leni and Wei, Zheng and Huang, Rong and Yip, David and Qu, Huamin},
title = {Is It the End? Guidelines for Cinematic Endings in Data Videos},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580701},
doi = {10.1145/3544548.3580701},
abstract = {Data videos are becoming increasingly popular in society and academia. Yet little is known about how to create endings that strengthen a lasting impression and persuasion. To fulfill the gap, this work aims to develop guidelines for data video endings by drawing inspiration from cinematic arts. To contextualize cinematic endings in data videos, 111 film endings and 105 data video endings are first analyzed to identify four common styles using the framework of ending punctuation marks. &nbsp;We conducted expert interviews (N=11) and formulated 20 guidelines for creating cinematic endings in data videos. To validate our guidelines, we conducted a user study where 24 participants were invited to design endings with and without our guidelines, which are evaluated by experts and the general public. The participants praise the clarity and usability of the guidelines, and results show that the endings with guidelines are perceived to be more understandable, impressive, and reflective.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {171},
numpages = {16},
keywords = {Visualization, Data Video, Interview, Lab Study, Storytelling, Guideline},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581452,
author = {Li, Wenchao and Sch\"{o}ttler, Sarah and Scott-Brown, James and Wang, Yun and Chen, Siming and Qu, Huamin and Bach, Benjamin},
title = {NetworkNarratives: Data Tours for Visual Network Exploration and Analysis},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581452},
doi = {10.1145/3544548.3581452},
abstract = {This paper introduces semi-automatic data tours to aid the exploration of complex networks. Exploring networks requires significant effort and expertise and can be time-consuming and challenging. Distinct from guidance and recommender systems for visual analytics, we provide a set of goal-oriented tours for network overview, ego-network analysis, community exploration, and other tasks. Based on interviews with five network analysts, we developed a user interface (NetworkNarratives) and 10 example tours. The interface allows analysts to navigate an interactive slideshow featuring facts about the network using visualizations and textual annotations. On each slide, an analyst can freely explore the network and specify nodes, links, or subgraphs as seed elements for follow-up tours. Two studies, comprising eight expert and 14 novice analysts, show that data tours reduce exploration effort, support learning about network exploration, and can aid the dissemination of analysis results. NetworkNarratives is available online, together with detailed illustrations for each tour.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {172},
numpages = {15},
keywords = {Guided exploration, network visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580965,
author = {Li, Haotian and Ying, Lu and Zhang, Haidong and Wu, Yingcai and Qu, Huamin and Wang, Yun},
title = {Notable: On-the-Fly Assistant for Data Storytelling in Computational Notebooks},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580965},
doi = {10.1145/3544548.3580965},
abstract = {Computational notebooks are widely used for data analysis. Their interleaved displays of code and execution results (e.g., visualizations) are welcomed since they enable iterative analysis and preserve the exploration process. However, the communication of data findings remains challenging in computational notebooks. Users have to carefully identify useful findings from useless ones, document them with texts and visual embellishments, and then organize them in different tools. Such workflow greatly increases their workload, according to our interviews with practitioners. To address the challenge, we designed Notable to offer on-the-fly assistance for data storytelling in computational notebooks. It provides intelligent support to minimize the work of documenting and organizing data findings and diminishes the cost of switching between data exploration and storytelling. To evaluate Notable, we conducted a user study with 12 data workers. The feedback from user study participants verifies its effectiveness and usability.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {173},
numpages = {16},
keywords = {data visualization, data storytelling, computational notebooks},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581330,
author = {Markant, Douglas and Rogha, Milad and Karduni, Alireza and Wesslen, Ryan and Dou, Wenwen},
title = {When Do Data Visualizations Persuade? The Impact of Prior Attitudes on Learning about Correlations from Scatterplot Visualizations},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581330},
doi = {10.1145/3544548.3581330},
abstract = {Data visualizations are vital to scientific communication on critical issues such as public health, climate change, and socioeconomic policy. They are often designed not just to inform, but to persuade people to make consequential decisions (e.g., to get vaccinated). Are such visualizations persuasive, especially when audiences have beliefs and attitudes that the data contradict? In this paper we examine the impact of existing attitudes (e.g., positive or negative attitudes toward COVID-19 vaccination) on changes in beliefs about statistical correlations when viewing scatterplot visualizations with different representations of statistical uncertainty. We find that strong prior attitudes are associated with smaller belief changes when presented with data that contradicts existing views, and that visual uncertainty representations may amplify this effect. Finally, even when participants’ beliefs about correlations shifted their attitudes remained unchanged, highlighting the need for further research on whether data visualizations can drive longer-term changes in views and behavior.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {174},
numpages = {16},
keywords = {uncertainty communication, data visualization, science communication, attitudes, persuasion},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581336,
author = {Brand, Nico and Odom, William and Barnett, Samuel},
title = {Envisioning and Understanding Orientations to Introspective AI: Exploring a Design Space with Meta.Aware},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581336},
doi = {10.1145/3544548.3581336},
abstract = {Introspection is the practice of looking inward for ongoing self-examination. It involves considering one's past experiences and asking questions about the present and future. Our work investigates how AI could open new possibilities for supporting introspective experiences. Adopting a design fiction approach, we created a fictional company called Meta.Aware to contextualize 4 different Introspective AI product concepts in the form of video sketches. We used the Meta.Aware platform to conduct interviews with 17 participants, using the 4 concept videos as prompts for discussion. Participants had a range of reactions related to perceived benefits and tensions in this emerging design space. We interpret these results to outline future design directions for mobilizing AI as a resource to support introspective experiences over time, as well as to reflect on issues and dilemmas bound to this emerging design space.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {175},
numpages = {18},
keywords = {AI, Research through Design, Introspection, Design Fiction, Personal Data},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580732,
author = {Carvalho, Arthur and Zavolokina, Liudmila and Bhunia, Suman and Chaudhary, Monu and Yoganathan, Nitharsan},
title = {Promoting Inclusiveness and Fairness through NFTs: The Case of Student-Athletes and NILs},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580732},
doi = {10.1145/3544548.3580732},
abstract = {Recent regulatory changes have enabled NCAA student-athletes to profit from their name, image, and likeness (NIL), departing from previous policies requiring those athletes to maintain their amateur status. However, despite the changes, it is unlikely that all the approximately 500,000 NCAA student-athletes will profit from NIL contracts. Within this context, we study how to design a fair and inclusive solution that may help all student-athletes secure NIL financial resources. Following a design science approach, we define design requirements after interviewing student-athletes. Subsequently, we derive three design principles: inclusiveness, fairness, and transparency. Thereafter, we suggest a blockchain-based artifact that satisfies all design principles. Our idea lies in designing collectibles as non-fungible tokens (NFTs) that pay different royalties whenever a transaction (purchase or exchange) happens in different markets (primary or secondary). Finally, we evaluate our solution by discussing its features with current student-athletes.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {176},
numpages = {9},
keywords = {inclusiveness, fairness, NFT, design science, blockchain},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581540,
author = {Boucher, Andy},
title = {Research Products at Scale: Learnings from Designing Devices in Multiples of Ones, Tens, Hundreds and Thousands},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581540},
doi = {10.1145/3544548.3581540},
abstract = {In this paper, I argue that scale is an important quality of research products [32] and reflect upon the lessons learnt from designing and producing research products at scales ranging from one-offs to those reproduced in thousands. I describe details from a body of research completed by our studio over twenty years by examining four research projects that were designed, developed, and manufactured at four distinct levels of scale. I draw out details from these projects that have not previously been reported and discuss the methodological implications of growth not just for design and production, but also for strategies for engaging with participants. In addition, I discuss particular features of research products produced at the four levels of scale and describe the benefits and trade-offs of producing research products at different scales.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {177},
numpages = {15},
keywords = {research through design, IoT, design research, self-build, research products},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581175,
author = {Benjamin, Jesse Josua and Biggs, Heidi and Berger, Arne and Rukanskaitundefined, Julija and Heidt, Michael B. and Merrill, Nick and Pierce, James and Lindley, Joseph},
title = {The Entoptic Field Camera as Metaphor-Driven Research-through-Design with AI Technologies},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581175},
doi = {10.1145/3544548.3581175},
abstract = {Artificial intelligence (AI) technologies are widely deployed in smartphone photography; and prompt-based image synthesis models have rapidly become commonplace. In this paper, we describe a Research-through-Design (RtD) project which explores this shift in the means and modes of image production via the creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer to perceptions of floaters or bright blue dots stemming from the physiological interplay of the eye and brain. We use the term entoptic as a metaphor to investigate how the material interplay of data and models in AI technologies shapes human experiences of reality. Through our case study using first-person design and a field study, we offer implications for critical, reflective, more-than-human and ludic design to engage AI technologies; the conceptualisation of an RtD research space which contributes to AI literacy discourses; and outline a research trajectory concerning materiality and design affordances of AI technologies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {178},
numpages = {19},
keywords = {artificial intelligence, materiality, GAN, research through design, technological mediation, image synthesis},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581432,
author = {Kowalczyk, Monica and Gunawan, Johanna T. and Choffnes, David and Dubois, Daniel J and Hartzog, Woodrow and Wilson, Christo},
title = {Understanding Dark Patterns in Home IoT Devices},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581432},
doi = {10.1145/3544548.3581432},
abstract = {Internet-of-Things (IoT) devices are ubiquitous, but little attention has been paid to how they may incorporate dark patterns despite consumer protections and privacy concerns arising from their unique access to intimate spaces and always-on capabilities. This paper conducts a systematic investigation of dark patterns in 57 popular, diverse smart home devices. We update manual interaction and annotation methods for the IoT context, then analyze dark pattern frequency across device types, manufacturers, and interaction modalities. We find that dark patterns are pervasive in IoT experiences, but manifest in diverse ways across device traits. Speakers, doorbells, and camera devices contain the most dark patterns, with manufacturers of such devices (Amazon and Google) having the most dark patterns compared to other vendors. We investigate how this distribution impacts the potential for consumer exposure to dark patterns, discuss broader implications for key stakeholders like designers and regulators, and identify opportunities for future dark patterns study.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {179},
numpages = {27},
keywords = {human factors, IoT, dark patterns, UX design},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581110,
author = {Song, Katherine Wei and Paulos, Eric},
title = {Vim: Customizable, Decomposable Electrical Energy Storage},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581110},
doi = {10.1145/3544548.3581110},
abstract = {Providing electrical power is essential for nearly all interactive technologies, yet it often remains an afterthought. Some designs handwave power altogether as an “exercise for later.” Others hastily string together batteries to meet the system’s electrical requirements, enclosing them in whatever box fits. Vim is a new approach – it elevates power as a first-class design element; it frees power from being a series of discrete elements, instead catering to exact requirements; it enables power to take on new, flexible forms; it is fabricated using low-cost, accessible materials and technologies; finally, it advances sustainability by being rechargeable, non-toxic, edible, and compostable. Vims are decomposable battery alternatives that rapidly charge and can power small applications for hours. We present Vims, detail their characteristics, offer design guidelines for their fabrication, and explore their use in applications spanning prototyping, fashion, and food, including novel systems that are entirely decomposable and edible.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {180},
numpages = {18},
keywords = {DIY, energy, biodegradation, supercapacitors, sustainability, decomposable materials},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581015,
author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
title = {Are Two Heads Better Than One in AI-Assisted Decision Making? Comparing the Behavior and Performance of Groups and Individuals in Human-AI Collaborative Recidivism Risk Assessment},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581015},
doi = {10.1145/3544548.3581015},
abstract = {With the prevalence of AI assistance in decision making, a more relevant question to ask than the classical question of “are two heads better than one?’’ is how groups’ behavior and performance in AI-assisted decision making compare with those of individuals’. In this paper, we conduct a case study to compare groups and individuals in human-AI collaborative recidivism risk assessment along six aspects, including decision accuracy and confidence, appropriateness of reliance on AI, understanding of AI, decision-making fairness, and willingness to take accountability. Our results highlight that compared to individuals, groups rely on AI models more regardless of their correctness, but they are more confident when they overturn incorrect AI recommendations. We also find that groups make fairer decisions than individuals according to the accuracy equality criterion, and groups are willing to give AI more credit when they make correct decisions. We conclude by discussing the implications of our work.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {348},
numpages = {18},
keywords = {Human-AI interaction, AI-assisted decision making, Group-AI interaction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580694,
author = {Gu, Hongyan and Yang, Chunxu and Haeri, Mohammad and Wang, Jing and Tang, Shirley and Yan, Wenzhong and He, Shujin and Williams, Christopher Kazu and Magaki, Shino and Chen, Xiang 'Anthony'},
title = {Augmenting Pathologists with NaviPath: Design and Evaluation of a Human-AI Collaborative Navigation System},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580694},
doi = {10.1145/3544548.3580694},
abstract = {Artificial Intelligence (AI) brings advancements to support pathologists in navigating high-resolution tumor images to search for pathology patterns of interest. However, existing AI-assisted tools have not realized this promised potential due to a lack of insight into pathology and HCI considerations for pathologists’ navigation workflows in practice. We first conducted a formative study with six medical professionals in pathology to capture their navigation strategies. By incorporating our observations along with the pathologists’ domain knowledge, we designed NaviPath&nbsp;— a human-AI collaborative navigation system. An evaluation study with 15 medical professionals in pathology indicated that: (i)&nbsp;compared to the manual navigation, participants saw more than twice the number of pathological patterns in unit time with NaviPath, and (ii)&nbsp;participants achieved higher precision and recall against the AI and the manual navigation on average. Further qualitative analysis revealed that navigation was more consistent with NaviPath, which can improve the overall examination quality.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {349},
numpages = {19},
keywords = {medical AI, navigation, Human-AI collaboration, digital pathology},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581282,
author = {Xu, Chengyuan and Lien, Kuo-Chin and H\"{o}llerer, Tobias},
title = {Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581282},
doi = {10.1145/3544548.3581282},
abstract = {When designing an AI-assisted decision-making system, there is often a tradeoff between precision and recall in the AI’s recommendations. We argue that careful exploitation of this tradeoff can harness the complementary strengths in the human-AI collaboration to significantly improve team performance. We investigate a real-world video anonymization task for which recall is paramount and more costly to improve. We analyze the performance of 78 professional annotators working with a) no AI assistance, b) a high-precision "restrained" AI, and c) a high-recall "zealous" AI in over 3,466 person-hours of annotation work. In comparison, the zealous AI helps human teammates achieve significantly shorter task completion time and higher recall. In a follow-up study, we remove AI assistance for everyone and find negative training effects on annotators trained with the restrained AI. These findings and our analysis point to important implications for the design of AI assistance in recall-demanding scenarios.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {350},
numpages = {15},
keywords = {video annotation, AI-assisted decision making, face detection, precision and recall, real-world application, empirical study, human-AI team, computer vision},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581131,
author = {Zheng, Chengbo and Wu, Yuheng and Shi, Chuhan and Ma, Shuai and Luo, Jiehui and Ma, Xiaojuan},
title = {Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581131},
doi = {10.1145/3544548.3581131},
abstract = {Existing research on human-AI collaborative decision-making focuses mainly on the interaction between AI and individual decision-makers. There is a limited understanding of how AI may perform in group decision-making. This paper presents a wizard-of-oz study in which two participants and an AI form a committee to rank three English essays. One novelty of our study is that we adopt a speculative design by endowing AI equal power to humans in group decision-making. We enable the AI to discuss and vote equally with other human members. We find that although the voice of AI is considered valuable, AI still plays a secondary role in the group because it cannot fully follow the dynamics of the discussion and make progressive contributions. Moreover, the divergent opinions of our participants regarding an “equal AI” shed light on the possible future of human-AI relations.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {351},
numpages = {19},
keywords = {automated essay grading, human-AI collaboration, group decision-making, qualitative study},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580672,
author = {Danry, Valdemar and Pataranutaporn, Pat and Mao, Yaoli and Maes, Pattie},
title = {Don’t Just Tell Me, Ask Me: AI Systems That Intelligently Frame Explanations as Questions Improve Human Logical Discernment Accuracy over Causal AI Explanations},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580672},
doi = {10.1145/3544548.3580672},
abstract = {Critical thinking is an essential human skill. Despite the importance of critical thinking, research reveals that our reasoning ability suffers from personal biases and cognitive resource limitations, leading to potentially dangerous outcomes. This paper presents the novel idea of AI-framed Questioning that turns information relevant to the AI classification into questions to actively engage users’ thinking and scaffold their reasoning process. We conducted a study with 204 participants comparing the effects of AI-framed Questioning on a critical thinking task; discernment of logical validity of socially divisive statements. Our results show that compared to no feedback and even causal AI explanations of an always correct system, AI-framed Questioning significantly increase human discernment of logically flawed statements. Our experiment exemplifies a future style of Human-AI co-reasoning system, where the AI becomes a critical thinking stimulator rather than an information teller.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {352},
numpages = {13},
keywords = {Language Model, Explainable AI, AI Explanation, Reasoning, Logic, AI, Human-AI Interaction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580983,
author = {He, Ziyao and Song, Yunpeng and Zhou, Shurui and Cai, Zhongmin},
title = {Interaction of Thoughts: Towards Mediating Task Assignment in Human-AI Cooperation with a Capability-Aware Shared Mental Model},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580983},
doi = {10.1145/3544548.3580983},
abstract = {The existing work on task assignment of human-AI cooperation did not consider the differences between individual team members regarding their capabilities, leading to sub-optimal task completion results. In this work, we propose a capability-aware shared mental model (CASMM) with the components of task grouping and negotiation, which utilize tuples to break down tasks into sets of scenarios relating to difficulties and then dynamically merge the task grouping ideas raised by human and AI through negotiation. We implement a prototype system and a 3-phase user study for the proof of concept via an image labeling task. The result shows building CASMM boosts the accuracy and time efficiency significantly through forming the task assignment close to real capabilities within few iterations. It helps users better understand the capability of AI and themselves. Our method has the potential to generalize to other scenarios such as medical diagnoses and automatic driving in facilitating better human-AI cooperation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {353},
numpages = {18},
keywords = {task assignment, shared mental model, human-AI cooperation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581095,
author = {Cabitza, Federico and Campagner, Andrea and Angius, Riccardo and Natali, Chiara and Reverberi, Carlo},
title = {AI Shall Have No Dominion: On How to Measure Technology Dominance in AI-Supported Human Decision-Making},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581095},
doi = {10.1145/3544548.3581095},
abstract = {In this article, we propose a conceptual and methodological framework for measuring the impact of the introduction of AI systems in decision settings, based on the concept of technological dominance, i.e. the influence that an AI system can exert on human judgment and decisions. We distinguish between a negative component of dominance (automation bias) and a positive one (algorithm appreciation) by focusing on and systematizing the patterns of interaction between human judgment and AI support, or reliance patterns, and their associated cognitive effects. We then define statistical approaches for measuring these dimensions of dominance, as well as corresponding qualitative visualizations. By reporting about four medical case studies, we illustrate how the proposed methods can be used to inform assessments of dominance and of related cognitive biases in real-world settings. Our study lays the groundwork for future investigations into the effects of introducing AI support into naturalistic and collaborative decision-making.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {354},
numpages = {20},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581225,
author = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
title = {Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581225},
doi = {10.1145/3544548.3581225},
abstract = {Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron’s usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report reflections both from our interviewees and from independent reviewers who critiqued performances of several of the scripts to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations—including plagiarism and bias—and participatory models for the design and deployment of such tools.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {355},
numpages = {34},
keywords = {improvisation, co-creativity, natural language generation, natural language evaluation, computational creativity, theatre, human-computer interaction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580900,
author = {Yildirim, Nur and Pushkarna, Mahima and Goyal, Nitesh and Wattenberg, Martin and Vi\'{e}gas, Fernanda},
title = {Investigating How Practitioners Use Human-AI Guidelines: A Case Study on the People + AI Guidebook},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580900},
doi = {10.1145/3544548.3580900},
abstract = {Artificial intelligence (AI) presents new challenges for the user experience (UX) of products and services. Recently, practitioner-facing resources and design guidelines have become available to ease some of these challenges. However, little research has investigated if and how these guidelines are used, and how they impact practice. In this paper, we investigated how industry practitioners use the People + AI Guidebook. We conducted interviews with 31 practitioners (i.e., designers, product managers) to understand how they use human-AI guidelines when designing AI-enabled products. Our findings revealed that practitioners use the guidebook not only for addressing AI’s design challenges, but also for education, cross-functional communication, and for developing internal resources. We uncovered that practitioners desire more support for early phase ideation and problem formulation to avoid AI product failures. We discuss the implications for future resources aiming to help practitioners in designing AI products.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {356},
numpages = {13},
keywords = {people AI guidebook, human-AI guidelines, human-AI interaction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580640,
author = {Li, Zhi and Ko, Yu-Jung and Putkonen, Aini and Feiz, Shirin and Ashok, Vikas and Ramakrishnan, Iv and Oulasvirta, Antti and Bi, Xiaojun},
title = {Modeling Touch-Based Menu Selection Performance of Blind Users via Reinforcement Learning},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580640},
doi = {10.1145/3544548.3580640},
abstract = {Although menu selection has been extensively studied in HCI, most existing studies have focused on sighted users, leaving blind users’ menu selection under-studied. In this paper, we propose a computational model that can simulate blind users’ menu selection performance and strategies, including the way they use techniques like swiping, gliding, and direct touch. We assume that selection behavior emerges as an adaptation to the user’s memory of item positions based on experience and feedback from the screen reader. A key aspect of our model is a model of long-term memory, predicting how a user recalls and forgets item position based on previous menu selections. We compare simulation results predicted by our model against data obtained in an empirical study with ten blind users. The model correctly simulated the effect of the menu length and menu arrangement on selection time, the action composition, and the menu selection strategy of the users.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {357},
numpages = {18},
keywords = {menu selection, boundedly optimal control, computational rationality, accessibility, deep reinforcement learning},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580926,
author = {Fruchard, Bruno and Malacria, Sylvain and Casiez, G\'{e}ry and Huot, St\'{e}phane},
title = {User Preference and Performance Using Tagging and Browsing for Image Labeling},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580926},
doi = {10.1145/3544548.3580926},
abstract = {Visual content must be labeled to facilitate navigation and retrieval, or provide ground truth data for supervised machine learning approaches. The efficiency of labeling techniques is crucial to produce numerous qualitative labels, but existing techniques remain sparsely evaluated. We systematically evaluate the efficiency of tagging and browsing tasks in relation to the number of images displayed, interaction modes, and the image visual complexity. Tagging consists in focusing on a single image to assign multiple labels (image-oriented strategy), and browsing in focusing on a single label to assign to multiple images (label-oriented strategy). In a first experiment, we focus on the nudges inducing participants to adopt one of the strategies (n=18). In a second experiment, we evaluate the efficiency of the strategies (n=24). Results suggest an image-oriented strategy (tagging task) leads to shorter annotation times, especially for complex images, and participants tend to adopt it regardless of the conditions they face.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {358},
numpages = {13},
keywords = {browsing, image labeling, user performance, tagging, visual complexity, Human-Computer Interaction, empirical studies, open science},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580959,
author = {Capel, Tara and Brereton, Margot},
title = {What is Human-Centered about Human-Centered AI? A Map of the Research Landscape},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580959},
doi = {10.1145/3544548.3580959},
abstract = {The application of Artificial Intelligence (AI) across a wide range of domains comes with both high expectations of its benefits and dire predictions of misuse. While AI systems have largely been driven by a technology-centered design approach, the potential societal consequences of AI have mobilized both HCI and AI researchers towards researching human-centered artificial intelligence (HCAI). However, there remains considerable ambiguity about what it means to frame, design and evaluate HCAI. This paper presents a critical review of the large corpus of peer-reviewed literature emerging on HCAI in order to characterize what the community is defining as HCAI. Our review contributes an overview and map of HCAI research based on work that explicitly mentions the terms ‘human-centered artificial intelligence’ or ‘human-centered machine learning’ or their variations, and suggests future challenges and research directions. The map reveals the breadth of research happening in HCAI, established clusters and the emerging areas of Interaction with AI and Ethical AI. The paper contributes a new definition of HCAI, and calls for greater collaboration between AI and HCI research, and new HCAI constructs.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {359},
numpages = {23},
keywords = {artificial intelligence, machine learning, human-centered artificial intelligence, human-centered machine learning, critical review},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580676,
author = {Pan, Lihang and Yu, Chun and He, Zhe and Shi, Yuanchun},
title = {A Human-Computer Collaborative Editing Tool for Conceptual Diagrams},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580676},
doi = {10.1145/3544548.3580676},
abstract = {Editing (e.g., editing conceptual diagrams) is a typical office task that requires numerous tedious GUI operations, resulting in poor interaction efficiency and user experience, especially on mobile devices. In this paper, we present a new type of human-computer collaborative editing tool (CET) that enables accurate and efficient editing with little interaction effort. CET divides the task into two parts, and the human and the computer focus on their respective specialties: the human describes high-level editing goals with multimodal commands, while the computer calculates, recommends, and performs detailed operations. We conducted a formative study (N = 16) to determine the concrete task division and implemented the tool on Android devices for the specific tasks of editing concept diagrams. The user study (N = 24 + 20) showed that it increased diagram editing speed by 32.75% compared with existing state-of-the-art commercial tools and led to better editing results and user experience.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {360},
numpages = {29},
keywords = {multi-modal interaction, natural content editing, conceptual diagram, human-computer collaboration},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581224,
author = {Lea, Colin and Huang, Zifang and Narain, Jaya and Tooley, Lauren and Yee, Dianna and Tran, Dung Tien and Georgiou, Panayiotis and Bigham, Jeffrey P and Findlater, Leah},
title = {From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581224},
doi = {10.1145/3544548.3581224},
abstract = {Consumer speech recognition systems do not work as well for many people with speech differences, such as stuttering, relative to the rest of the general population. However, what is not clear is the degree to which these systems do not work, how they can be improved, or how much people want to use them. In this paper, we first address these questions using results from a 61-person survey from people who stutter and find participants want to use speech recognition but are frequently cut off, misunderstood, or speech predictions do not represent intent. In a second study, where 91 people who stutter recorded voice assistant commands and dictation, we quantify how dysfluencies impede performance in a consumer-grade speech recognition system. Through three technical investigations, we demonstrate how many common errors can be prevented, resulting in a system that cuts utterances off 79.1% less often and improves word error rate from 25.4% to 9.9%.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {361},
numpages = {16},
keywords = {accessibility, speech input, voice assistants, dictation, stuttering},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581352,
author = {Gebreegziabher, Simret Araya and Zhang, Zheng and Tang, Xiaohang and Meng, Yihao and Glassman, Elena L. and Li, Toby Jia-Jun},
title = {PaTAT: Human-AI Collaborative Qualitative Coding with Explainable Interactive Rule Synthesis},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581352},
doi = {10.1145/3544548.3581352},
abstract = {Over the years, the task of AI-assisted data annotation has seen remarkable advancements. However, a specific type of annotation task, the qualitative coding performed during thematic analysis, has characteristics that make effective human-AI collaboration difficult. Informed by a formative study, we designed PaTAT, a new AI-enabled tool that uses an interactive program synthesis approach to learn flexible and expressive patterns over user-annotated codes in real-time as users annotate data. To accommodate the ambiguous, uncertain, and iterative nature of thematic analysis, the use of user-interpretable patterns allows users to understand and validate what the system has learned, make direct fixes, and easily revise, split, or merge previously annotated codes. This new approach also helps human users to learn data characteristics and form new theories in addition to facilitating the “learning” of the AI model. PaTAT’s usefulness and effectiveness were evaluated in a lab user study.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {362},
numpages = {19},
keywords = {human-AI collaboration, data annotation, qualitative analysis},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580699,
author = {Sakashita, Mose and Zhang, Ruidong and Li, Xiaoyi and Kim, Hyunju and Russo, Michael and Zhang, Cheng and Jung, Malte F. and Guimbreti\`{e}re, Fran\c{c}ois},
title = {ReMotion: Supporting Remote Collaboration in Open Space with Automatic Robotic Embodiment},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580699},
doi = {10.1145/3544548.3580699},
abstract = {Design activities, such as brainstorming or critique, often take place in open spaces combining whiteboards and tables to present artefacts. In co-located settings, peripheral awareness enables participants to understand each other’s locus of attention with ease. However, these spatial cues are mostly lost while using videoconferencing tools. Telepresence robots could bring back a sense of presence, but controlling them is distracting. To address this problem, we present ReMotion, a fully automatic robotic proxy designed to explore a new way of supporting non-collocated open-space design activities. ReMotion combines a commodity body tracker (Kinect) to capture a user’s location and orientation over a wide area with a minimally invasive wearable system (NeckFace) to capture facial expressions. Due to its omnidirectional platform, ReMotion embodiment can render a wide range of body movements. A formative evaluation indicated that our system enhances the sharing of attention and the sense of co-presence enabling seamless movement-in-space during a design review task.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {363},
numpages = {14},
keywords = {Remote Collaboration;, Robotic Embodiment, Telepresence Robot},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580753,
author = {Wang, Fengjie and Liu, Xuye and Liu, Oujing and Neshati, Ali and Ma, Tengfei and Zhu, Min and Zhao, Jian},
title = {Slide4N: Creating Presentation Slides from Computational Notebooks with Human-AI Collaboration},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580753},
doi = {10.1145/3544548.3580753},
abstract = {Data scientists often have to use other presentation tools (e.g., Microsoft PowerPoint) to create slides to communicate their analysis obtained using computational notebooks. Much tedious and repetitive work is needed to transfer the routines of notebooks (e.g., code, plots) to the presentable contents on slides (e.g., bullet points, figures). We propose a human-AI collaborative approach and operationalize it within Slide4N, an interactive AI assistant for data scientists to create slides from computational notebooks. Slide4N leverages advanced natural language processing techniques to distill key information from user-selected notebook cells and then renders them in appropriate slide layouts. The tool also provides intuitive interactions that allow further refinement and customization of the generated slides. We evaluated Slide4N with a two-part user study, where participants appreciated this human-AI collaborative approach compared to fully-manual or fully-automatic methods. The results also indicate the usefulness and effectiveness of Slide4N in slide creation tasks from notebooks.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {364},
numpages = {18},
keywords = {data science., human-AI collaboration, natural language processing, slides generation, computational notebooks},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581148,
author = {Hu, Erzhen and Gr\o{}nb\ae{}k, Jens Emil Sloth and Ying, Wen and Du, Ruofei and Heo, Seongkook},
title = {ThingShare: Ad-Hoc Digital Copies of Physical Objects for Sharing Things in Video Meetings},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581148},
doi = {10.1145/3544548.3581148},
abstract = {In video meetings, individuals may wish to share various physical objects with remote participants, such as physical documents, design prototypes, and personal belongings. However, our formative study discovered that this poses several challenges, including difficulties in referencing a remote user’s physical objects, the limited visibility of the object, and the friction of properly framing and orienting an object to the camera. To address these challenges, we propose ThingShare, a video-conferencing system designed to facilitate the sharing of physical objects during remote meetings. With ThingShare, users can quickly create digital copies of physical objects in the video feeds, which can then be magnified on a separate panel for focused viewing, overlaid on the user’s video feed for sharing in context, and stored in the object drawer for reviews. Our user study demonstrated that ThingShare made initiating object-centric conversations more efficient and provided a more stable and comprehensive view of shared objects.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {365},
numpages = {22},
keywords = {augmented communication, collaborative work, shared task space, object-centered meetings, video-mediated communication},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580895,
author = {Wang, Bryan and Li, Gang and Li, Yang},
title = {Enabling Conversational Interaction with Mobile UI Using Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580895},
doi = {10.1145/3544548.3580895},
abstract = {Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {432},
numpages = {17},
keywords = {Conversational Interaction, Mobile UI, Large Language Models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580688,
author = {H\"{a}m\"{a}l\"{a}inen, Perttu and Tavast, Mikke and Kunnari, Anton},
title = {Evaluating Large Language Models in Generating Synthetic HCI Research Data: A Case Study},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580688},
doi = {10.1145/3544548.3580688},
abstract = {Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {433},
numpages = {19},
keywords = {GPT-3, User models, Language models, User experience},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580940,
author = {Mcnutt, Andrew M and Wang, Chenglong and Deline, Robert A and Drucker, Steven M.},
title = {On the Design of AI-Powered Code Assistants for Notebooks},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580940},
doi = {10.1145/3544548.3580940},
abstract = {AI-powered code assistants, such as Copilot, are quickly becoming a ubiquitous component of contemporary coding contexts. Among these environments, computational notebooks, such as Jupyter, are of particular interest as they provide rich interface affordances that interleave code and output in a manner that allows for both exploratory and presentational work. Despite their popularity, little is known about the appropriate design of code assistants in notebooks. We investigate the potential of code assistants in computational notebooks by creating a design space (reified from a survey of extant tools) and through an interview-design study (with 15 practicing data scientists). Through this work, we identify challenges and opportunities for future systems in this space, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specific tools (like linters), and the importance of polite assistants.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {434},
numpages = {16},
keywords = {Computational Notebooks, Copilot, Design Probe, Code Assistant, Artificial Intelligence},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580948,
author = {Wang, Sitong and Petridis, Savvas and Kwon, Taeahn and Ma, Xiaojuan and Chilton, Lydia B},
title = {PopBlends: Strategies for Conceptual Blending with Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580948},
doi = {10.1145/3544548.3580948},
abstract = {Pop culture is an important aspect of communication. On social media people often post pop culture reference images that connect an event, product or other entity to a pop culture domain. Creating these images is a creative challenge that requires finding a conceptual connection between the users’ topic and a pop culture domain. In cognitive theory, this task is called conceptual blending. We present a system called PopBlends that automatically suggests conceptual blends. The system explores three approaches that involve both traditional knowledge extraction methods and large language models. Our annotation study shows that all three methods provide connections with similar accuracy, but with very different characteristics. Our user study shows that people found twice as many blend suggestions as they did without the system, and with half the mental demand. We discuss the advantages of combining large language models with knowledge bases for supporting divergent and convergent thinking.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {435},
numpages = {19},
keywords = {creativity support tools, applications of large language models, natural language processing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581318,
author = {Zhou, Jiawei and Zhang, Yixuan and Luo, Qianni and Parker, Andrea G and De Choudhury, Munmun},
title = {Synthetic Lies: Understanding AI-Generated Misinformation and Evaluating Algorithmic and Human Solutions},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581318},
doi = {10.1145/3544548.3581318},
abstract = {Large language models have abilities in creating high-volume human-like texts and can be used to generate persuasive misinformation. However, the risks remain under-explored. To address the gap, this work first examined characteristics of AI-generated misinformation (AI-misinfo) compared with human creations, and then evaluated the applicability of existing solutions. We compiled human-created COVID-19 misinformation and abstracted it into narrative prompts for a language model to output AI-misinfo. We found significant linguistic differences within human-AI pairs, and patterns of AI-misinfo in enhancing details, communicating uncertainties, drawing conclusions, and simulating personal tones. While existing models remained capable of classifying AI-misinfo, a significant performance drop compared to human-misinfo was observed. Results suggested that existing information assessment guidelines had questionable applicability, as AI-misinfo tended to meet criteria in evidence credibility, source transparency, and limitation acknowledgment. We discuss implications for practitioners, researchers, and journalists, as AI can create new challenges to the societal problem of misinformation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {436},
numpages = {20},
keywords = {COVID-19, AI-generated misinformation, large language model, responsible AI, generative AI, GPT, misinformation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581388,
author = {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
title = {Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581388},
doi = {10.1145/3544548.3581388},
abstract = {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {437},
numpages = {21},
keywords = {design tools, language models, end-users},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581236,
author = {Guo, Grace and Karavani, Ehud and Endert, Alex and Kwon, Bum Chul},
title = {Causalvis: Visualizations for Causal Inference},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581236},
doi = {10.1145/3544548.3581236},
abstract = {Causal inference is a statistical paradigm for quantifying causal effects using observational data. It is a complex process, requiring multiple steps, iterations, and collaborations with domain experts. Analysts often rely on visualizations to evaluate the accuracy of each step. However, existing visualization toolkits are not designed to support the entire causal inference process within computational environments familiar to analysts. In this paper, we address this gap with Causalvis, a Python visualization package for causal inference. Working closely with causal inference experts, we adopted an iterative design process to develop four interactive visualization modules to support causal inference analysis tasks. The modules are then presented back to the experts for feedback and evaluation. We found that Causalvis effectively supported the iterative causal inference process. We discuss the implications of our findings for designing visualizations for causal inference, particularly for tasks of communication and collaboration.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {462},
numpages = {20},
keywords = {design study, causality, causal inference},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581021,
author = {Yen, Chi-Hsien and Cheng, Haocong and Xia, Yilin and Huang, Yun},
title = {CrowdIDEA: Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581021},
doi = {10.1145/3544548.3581021},
abstract = {Causal reasoning is crucial for people to understand data, make decisions, or take action. However, individuals often have blind spots and overlook alternative hypotheses, and using only data is insufficient for causal reasoning. We designed and implemented CrowdIDEA, a novel tool consisting of a three-panel integration incorporating the crowd’s beliefs (Crowd Panel with two designs), data analytics (Data Panel), and user’s causal diagram (Diagram Panel) to stimulate causal reasoning. Through an experiment with 54 participants, we showed the significant effects of the Crowd Panel designs on the outcomes of causal reasoning, such as an increased number of causal beliefs generated. Participants also devised new strategies for bootstrapping, strengthening, deepening, and explaining their causal beliefs, as well as taking advantage of the unique characteristics of both qualitative and quantitative data sources to reduce potential biases in reasoning. Our work makes theoretical and design implications for exploratory causal reasoning.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {463},
numpages = {17},
keywords = {Causal Reasoning, Crowd Intelligence, Crowd-informed Reasoning Tools, Causal Diagrams, Visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581067,
author = {Song, Sicheng and Chen, Juntong and Li, Chenhui and Wang, Changbo},
title = {GVQA: Learning to Answer Questions about Graphs with Visualizations via Knowledge Base},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581067},
doi = {10.1145/3544548.3581067},
abstract = {Graphs are common charts used to represent the topological relationship between nodes. It is a powerful tool for data analysis and information retrieval tasks involve asking questions about graphs. In formative study, we found that questions for graphs are not only about the relationship of nodes but also about the properties of graph elements. We propose a pipeline to answer natural language questions about graph visualizations and generate visual answers. We first extract the data from graphs and convert them into GML format. We design data structures to encode graph information and convert them into an knowledge base. We then extract topic entities from questions. We feed questions, entities and knowledge bases into our question-answer model to obtain the SPARQL queries for textual answers. Finally, we design a module to present the answers visually. A user study demonstrates that these visual and textual answers are useful, credible and and transparent.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {464},
numpages = {16},
keywords = {Natural Language Process, Visualization, Reinforcement Learning, Knowledge Base, Question Answering, Network Graph},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580869,
author = {Kale, Alex and Lee, Sarah and Goan, Terrance and Tipton, Elizabeth and Hullman, Jessica},
title = {MetaExplorer : Facilitating Reasoning with Epistemic Uncertainty in Meta-Analysis},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580869},
doi = {10.1145/3544548.3580869},
abstract = {Scientists often use meta-analysis to characterize the impact of an intervention on some outcome of interest across a body of literature. However, threats to the utility and validity of meta-analytic estimates arise when scientists average over potentially important variations in context like different research designs. Uncertainty about quality and commensurability of evidence casts doubt on results from meta-analysis, yet existing software tools for meta-analysis do not provide an explicit software representation of these concerns. We present MetaExplorer, a prototype system for meta-analysis that we developed using iterative design with meta-analysis experts to provide a guided process for eliciting assessments of uncertainty and reasoning about how to incorporate them during statistical inference. Our qualitative evaluation of MetaExplorer with experienced meta-analysts shows that imposing a structured workflow both elevates the perceived importance of epistemic concerns and presents opportunities for tools to engage users in dialogue around goals and standards for evidence aggregation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {465},
numpages = {14},
keywords = {literature review, Meta-analysis, epistemic uncertainty},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580808,
author = {Koonchanok, Ratanond and Tawde, Gauri Yatindra and Narayanasamy, Gokul Ragunandhan and Walimbe, Shalmali and Reda, Khairi},
title = {Visual Belief Elicitation Reduces the Incidence of False Discovery},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580808},
doi = {10.1145/3544548.3580808},
abstract = {Visualization supports exploratory data analysis (EDA), but EDA frequently presents spurious charts, which can mislead people into drawing unwarranted conclusions. We investigate interventions to prevent false discovery from visualized data. We evaluate whether eliciting analyst beliefs helps guard against the over-interpretation of noisy visualizations. In two experiments, we exposed participants to both spurious and ‘true’ scatterplots, and assessed their ability to infer data-generating models that underlie those samples. Participants who underwent prior belief elicitation made 21% more correct inferences along with 12% fewer false discoveries. This benefit was observed across a variety of sample characteristics, suggesting broad utility to the intervention. However, additional interventions to highlight counterevidence and sample uncertainty did not provide significant advantage. Our findings suggest that lightweight, belief-driven interactions can yield a reliable, if moderate, reduction in false discovery. This work also suggests future directions to improve visual inference and reduce bias.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {466},
numpages = {17},
keywords = {belief elicitation, graphical inference., False discovery},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581218,
author = {Bancilhon, Melanie and Wright, Amanda and Ha, Sunwoo and Crouser, R. Jordan and Ottley, Alvitta},
title = {Why Combining Text and Visualization Could Improve Bayesian Reasoning: A Cognitive Load Perspective},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581218},
doi = {10.1145/3544548.3581218},
abstract = {Investigations into using visualization to improve Bayesian reasoning and advance risk communication have produced mixed results, suggesting that cognitive ability might affect how users perform with different presentation formats. Our work examines the cognitive load elicited when solving Bayesian problems using icon arrays, text, and a juxtaposition of text and icon arrays. We used a three-pronged approach to capture a nuanced picture of cognitive demand and measure differences in working memory capacity, performance under divided attention using a dual-task paradigm, and subjective ratings of self-reported effort. We found that individuals with low working memory capacity made fewer errors and experienced less subjective workload when the problem contained an icon array compared to text alone, showing that visualization improves accuracy while exerting less cognitive demand. We believe these findings can considerably impact accessible risk communication, especially for individuals with low working memory capacity.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {467},
numpages = {15},
keywords = {Decision-making, Bayesian reasoning, Perception and Cognitive Load},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581390,
author = {Hayatpur, Devamardeep and Wigdor, Daniel and Xia, Haijun},
title = {CrossCode: Multi-Level Visualization of Program Execution},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581390},
doi = {10.1145/3544548.3581390},
abstract = {Program visualizations help to form useful mental models of how programs work, and to reason and debug code. But these visualizations exist at a fixed level of abstraction, e.g., line-by-line. In contrast, programmers switch between many levels of abstraction when inspecting program behavior. Based on results from a formative study of hand-designed program visualizations, we designed CrossCode, a web-based program visualization system for JavaScript that leverages structural cues in syntax, control flow, and data flow to aggregate and navigate program execution across multiple levels of abstraction. In an exploratory qualitative study with experts, we found that CrossCode enabled participants to maintain a strong sense of place in program execution, was conducive to explaining program behavior, and helped track changes and updates to the program state.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {593},
numpages = {13},
keywords = {program visualization, programming, debugging},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581403,
author = {Jiang, Peiling and Sun, Fuling and Xia, Haijun},
title = {Log-It: Supporting Programming with Interactive, Contextual, Structured, and Visual Logs},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581403},
doi = {10.1145/3544548.3581403},
abstract = {Logging is a widely used technique for inspecting and understanding programs. However, the presentation of logs still often takes its ancient form of a linear stream of text that resides in a terminal, console, or log file. Despite its simplicity, interpreting log output is often challenging due to the large number of textual logs that lack structure and context. We conducted content analysis and expert interviews to understand the practices and challenges inherent in logging. These activities demonstrated that the current representation of logs does not provide the rich structures programmers need to interpret them or the program’s behavior. We present Log-it, a logging interface that enables programmers to interactively structure and visualize logs in situ. A user study with novices and experts showed that Log-it’s syntax and interface have a minimal learning curve, and the interactive representations and organizations of logs help programmers easily locate, synthesize, and understand logs.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {594},
numpages = {16},
keywords = {Program comprehension, Visualization, Programming support},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580785,
author = {Beckmann, Tom and Rein, Patrick and Ramson, Stefan and Bergsiek, Joana and Hirschfeld, Robert},
title = {Structured Editing for All: Deriving Usable Structured Editors from Grammars},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580785},
doi = {10.1145/3544548.3580785},
abstract = {Structured editing can show benefits in learnability, tool building, and editing efficiency in programming. However, creating a usable structured editor is laborious and demanding, typically requiring tool builders to manually create or adjust editing interactions. We present Sandblocks, a system that allows users to automatically generate structured editors for every language with a formal grammar available. Our system’s input reconciliation process acts on arbitrary syntax trees to provides consistent interactions across our generated editors. Our editors’ editing experience is designed to be familiar to users from textual editing but, compared to previous work, requires no manual annotation in the grammars. We demonstrate our editors’ usability across languages through a user study (N=18). Compared to conventional text editors, even with minimal training, participants only took on average 21% (JS), 34% (Clojure), and 95% (RegExp) longer and reported that editing felt natural with a score of 6/7.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {595},
numpages = {16},
keywords = {text-like editing, structured editing, grammars},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581516,
author = {Zhang, Ashley Ge and Chen, Yan and Oney, Steve},
title = {VizProg: Identifying Misunderstandings By Visualizing Students’ Coding Progress},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581516},
doi = {10.1145/3544548.3581516},
abstract = {Programming instructors often conduct in-class exercises to help them identify students that are falling behind and surface students’ misconceptions. However, as we found in interviews with programming instructors, monitoring students’ progress during exercises is difficult, particularly for large classes. We present VizProg, a system that allows instructors to monitor and inspect students’ coding progress in real-time during in-class exercises. VizProg represents students’ statuses as a 2D Euclidean spatial map that encodes the students’ problem-solving approaches and progress in real-time. VizProg allows instructors to navigate the temporal and structural evolution of students’ code, understand relationships between code, and determine when to provide feedback. A comparison experiment showed that VizProg helped to identify more students’ problems than a baseline system. VizProg also provides richer and more comprehensive information for identifying important student behavior. By managing students’ activities at scale, this work presents a new paradigm for improving the quality of live learning.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {596},
numpages = {16},
keywords = {code visualization, programming education at scale},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581436,
author = {Krings, Kevin and Bohn, Nino S. and Hille, Nora Anna Luise and Ludwig, Thomas},
title = {“What If Everyone is Able to Program?” – Exploring the Role of Software Development in Science Fiction},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581436},
doi = {10.1145/3544548.3581436},
abstract = {For decades, research around emerging technologies has been inspired by science fiction and vice versa. While so far almost only the technologies themselves have been considered, we explore the underlying software development and programming approaches. We therefore conduct a detailed media content analysis of twenty-seven movies that examines the role of software development in science fiction by identifying and investigating new approaches to programming and how software development is conceptualized portrayed within science fiction scenes. With the additional analysis of eighteen design fiction stories exploring the scenario “What if everyone is able to program?”, we envision potential impacts of the democratization of software development on business and society. Our study opens new discussions and perspectives, by investigating the current vision of the future of programming and uncovers new approaches to software development which can serve as a starting point for further research in the HCI community.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {597},
numpages = {13},
keywords = {Design Fiction, Content Analysis, Science Fiction, End-User Development (EUD), Software Development},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580817,
author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
title = {“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580817},
doi = {10.1145/3544548.3580817},
abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user’s natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users’ understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {598},
numpages = {31},
keywords = {Human-AI Interaction, Spreadsheets, Natural Language Programming, Large Language Models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581395,
author = {Jin, Qiao and Liu, Yu and Sun, Ruixuan and Chen, Chen and Zhou, Puqi and Han, Bo and Qian, Feng and Yarosh, Svetlana},
title = {Collaborative Online Learning with VR Video: Roles of Collaborative Tools and Shared Video Control},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581395},
doi = {10.1145/3544548.3581395},
abstract = {Virtual Reality (VR) has a noteworthy educational potential by providing immersive and collaborative environments. As an alternative but cost-effective way of delivering realistic environments in VR, using 360-degree videos in immersive VR (VR videos) received more attention. Although many studies reported positive learning experiences with VR videos, little is known about how collaborative learning performs on VR video viewing systems. In this study, we implemented two collaborative VR video viewing modes based on the way of group video control, synchronized or shared (Sync mode) and non-synchronized or individual (Non-sync mode) video control, against a conventional VR video viewing setting (Basic mode). We conducted a within-subject study (N = 54) in a lab-simulated remote learning environment. Our results show that collaborative VR video modes (Sync and Non-sync mode) improve users’ learning experiences and collaboration quality, especially with shared video control. Our findings provide directions for designing and employing collaborative VR video tools in online learning environments.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {713},
numpages = {18},
keywords = {collaborative learning, 360-degree video, Virtual Reality, social VR, educational VR},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580944,
author = {Fitton, Isabel and Clarke, Christopher and Dalton, Jeremy and Proulx, Michael J and Lutteroth, Christof},
title = {Dancing with the Avatars: Minimal Avatar Customisation Enhances Learning in a Psychomotor Task},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580944},
doi = {10.1145/3544548.3580944},
abstract = {Virtual environments can support psychomotor learning by allowing learners to observe instructor avatars. Instructor avatars that look like the learner hold promise in enhancing learning; however, it is unclear whether this works for psychomotor tasks and how similar avatars need to be. We investigated ‘minimal’ customisation of instructor avatars, approximating a learner’s appearance by matching only key visual features: gender, skin-tone, and hair colour. These avatars can be created easily and avoid problems of highly similar avatars. Using modern dancing as a skill to learn, we compared the effects of visually similar and dissimilar avatars, considering both learning on a screen (n=59) and in VR (n=38). Our results indicate that minimal avatar customisation leads to significantly more vivid visual imagery of the dance moves than dissimilar avatars. We analyse variables affecting interindividual differences, discuss the results in relation to theory, and derive design implications for psychomotor training in virtual environments.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {714},
numpages = {16},
keywords = {Avatar Customisation, Virtual Environments, Skills Training, Virtual Reality, Psychomotor},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581100,
author = {Clarke, Christopher and Xu, Jingnan and Zhu, Ye and Dharamshi, Karan and McGill, Harry and Black, Stephen and Lutteroth, Christof},
title = {FakeForward: Using Deepfake Technology for Feedforward Learning},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581100},
doi = {10.1145/3544548.3581100},
abstract = {Videos are commonly used to support learning of new skills, to improve existing skills, and as a source of motivation for training. Video self-modelling (VSM) is a learning technique that improves performance and motivation by showing a user a video of themselves performing a skill at a level they have not yet achieved. Traditional VSM is very data and labour intensive: a lot of video footage needs to be collected and manually edited in order to create an effective self-modelling video. We address this by presenting FakeForward – a method which uses deepfakes to create self-modelling videos from videos of other people. FakeForward turns videos of better-performing people into effective, personalised training tools by replacing their face with the user’s. We investigate how FakeForward can be effectively applied and demonstrate its efficacy in rapidly improving performance in physical exercises, as well as confidence and perceived competence in public speaking.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {715},
numpages = {17},
keywords = {Physical Exercise, Public Speaking, Skill Acquisition, Deepfake, Feedforward, Training, Videos, Fitness},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581572,
author = {Zegura, Cass and Shapiro, Ben Rydal and MacDonald, Robert and Borenstein, Jason and Zegura, Ellen},
title = {“Moment to Moment”: A Situated View of Teaching Ethics from the Perspective of Computing Ethics Teaching Assistants},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581572},
doi = {10.1145/3544548.3581572},
abstract = {The HCI research community has long centered ethics in HCI research and practice. This interest has persisted as scholars highlight the need for more situated understandings and deeper integration of ethics into HCI. In parallel, HCI scholars and students have become increasingly involved in teaching computing ethics across many different university contexts, bringing in valuable perspectives informed by the connections between HCI and the socio-technical subject matter of computing ethics. Yet explicitly bringing these two threads together – examining the teaching of ethics through an HCI research lens – remains nascent. This paper integrates work in HCI and computing education to focus on the role and experience of computing ethics teaching assistants (CETAs), who are increasingly involved in ethics instruction and whose perspectives are predominantly missing in existing literature spanning HCI and computing education. Drawing on HCI theories and methods, our qualitative study of eleven CETAs at two American universities makes three contributions to the HCI literature. First, we build an understanding of who these TAs are with respect to the unique position of teaching computing ethics. Second, we characterize how CETAs’ teaching and learning is situated and shaped within different communities and institutional contexts. Finally, we suggest several implications for the design of ethics instruction within undergraduate computing programs. More broadly, our work can be viewed as a call to action, encouraging HCI scholars to play a more significant role in studying and designing the teaching and learning of computing ethics.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {716},
numpages = {15},
keywords = {Teaching assistants, Situated teaching and learning, Computing ethics, Community of practice},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580892,
author = {Amores Fernandez, Judith and Mehra, Nirmita and Rasch, Bjoern and Maes, Pattie},
title = {Olfactory Wearables for Mobile Targeted Memory Reactivation},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580892},
doi = {10.1145/3544548.3580892},
abstract = {This paper investigates how a smartphone-controlled olfactory wearable might improve memory recall. We conducted a within-subjects experiment with 32 participants using the device and without (control). In the experimental condition, bursts of odor were released during visuo-spatial memory navigation tasks, and replayed during sleep the following night in the subjects’ home. We found that compared to control, there was an improvement in memory performance when using the scent wearable in memory tasks that involved walking in a physical space. Furthermore, participants recalled more objects and translations when re-exposed to the same scent during the recall test, in addition to during sleep. These effects were statistically significant, and, in the object recall task, they also persisted for more than one week. This experiment demonstrates a potential practical application of olfactory interfaces that can interact with a user during wake as well as sleep to support memory.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {717},
numpages = {20},
keywords = {olfaction, olfactory interfaces, well-being, Targeted Memory Reactivation, sleep, learning, Memory, wearables},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581364,
author = {Rong, Ethan Z. and Zhou, Mo Morgana and Gao, Ge and Lu, Zhicong},
title = {Understanding Personal Data Tracking and Sensemaking Practices for Self-Directed Learning in Non-Classroom and Non-Computer-Based Contexts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581364},
doi = {10.1145/3544548.3581364},
abstract = {Self-directed learning is becoming a significant skill for learners. However, learners may suffer from difficulties such as distractions, a lack of motivation, and so on. While self-tracking technologies have the potential to address these challenges, existing tools and systems mainly focused on tracking computer-based learning data in classroom contexts. Little is known about how students track and make sense of their learning data from non-classroom learning activities and which types of learning data are personally meaningful for learners. In this paper, we conducted a qualitative study with 24 users of Timing, a mobile learning tracking application in China. Our findings indicated that users tracked a variety of qualitative learning data (e.g., videos, photos of learning materials, and emotions) and made sense of this data using different strategies such as observing behavioral and contextual details in videos. We then provided implications for designing non-classroom and non-computer-based personal learning tracking tools.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {718},
numpages = {16},
keywords = {behavior change, personal informatics, non-classroom-based learning, self-directed learning, self-tracking, non-computer-based learning},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581269,
author = {Cui, Wenzhe and Liu, Rui and Li, Zhi and Wang, Yifan and Wang, Andrew and Zhao, Xia and Rashidian, Sina and Baig, Furqan and Ramakrishnan, IV and Wang, Fusheng and Bi, Xiaojun},
title = {GlanceWriter: Writing Text by Glancing Over Letters with Gaze},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581269},
doi = {10.1145/3544548.3581269},
abstract = {Writing text with eye gaze only is an appealing hands-free text entry method. However, existing gaze-based text entry methods introduce eye fatigue and are slow in typing speed because they often require users to dwell on letters of a word, or mark the starting and ending positions of a gaze path with extra operations for entering a word. In this paper, we propose GlanceWriter, a text entry method that allows users to enter text by glancing over keys one by one without any need to dwell on any keys or specify the starting and ending positions of a gaze path when typing a word. To achieve so, GlanceWriter probabilistically determines the letters to be typed based on the dynamics of gaze movements and gaze locations. Our user studies demonstrate that GlanceWriter significantly improves the text entry performance over EyeSwipe, a dwell-free input method using “reverse crossing” to identify the starting and ending keys. GlanceWriter also outperforms the dwell-free gaze input method of Tobii’s Communicator 5, a commercial eye gaze-based communication system. Overall, GlanceWriter achieves dwell-free and crossing-free text entry by probabilistically decoding gaze paths, offering a promising gaze-based text entry method.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {719},
numpages = {13},
keywords = {gesture input, text input, word gesture, eye gaze},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581430,
author = {Zhou, Chen and Fennedy, Katherine and Tan, Felicia Fang-Yi and Zhao, Shengdong and Shao, Yurui},
title = {Not All Spacings Are Created Equal: The Effect of Text Spacings in On-the-Go Reading Using Optical See-Through Head-Mounted Displays},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581430},
doi = {10.1145/3544548.3581430},
abstract = {The emergent Optical Head-Mounted Display (OHMD) platform has made mobile reading possible by superimposing digital text onto users’ view of the environment. However, mobile reading through OHMD needs to be effectively balanced with the user’s environmental awareness. Hence, a series of studies were conducted to explore how text spacing strategies facilitate such balance. Through these studies, it was found that increasing spacing within the text can significantly enhance mobile reading on OHMDs in both simple and complex navigation scenarios and that such benefits mainly come from increasing the inter-line spacing, but not inter-word spacing. Compared with existing positioning strategies, increasing inter-line spacing improves mobile OHMD information reading in terms of reading speed (11.9% faster), walking speed (3.7% faster), and switching between reading and navigation (106.8% more accurate and 33% faster).},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {720},
numpages = {19},
keywords = {Smart Glasses, Inter-line Spacing, OHMD, Mobile Reading, Reading on-the-go, Text Spacing, Heads-up Computing, Inter-word Spacing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581065,
author = {Cai, Runze and Janaka, Nuwan Nanayakkarawasam Peru Kandage and Zhao, Shengdong and Sun, Minghui},
title = {ParaGlassMenu: Towards Social-Friendly Subtle Interactions in Conversations},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581065},
doi = {10.1145/3544548.3581065},
abstract = {Interactions with digital devices during social settings can reduce social engagement and interrupt conversations. To overcome these drawbacks, we designed ParaGlassMenu, a semi-transparent circular menu that can be displayed around a conversation partner’s face on Optical See-Through Head-Mounted Display (OHMD) and interacted subtly using a ring mouse. We evaluated ParaGlassMenu with several alternative approaches (Smartphone, Voice assistant, and Linear OHMD menus) by manipulating Internet-of-Things (IoT) devices in a simulated conversation setting with a digital partner. Results indicated that the ParaGlassMenu offered the best overall performance in balancing social engagement and digital interaction needs in conversations. To validate these findings, we conducted a second study in a realistic conversation scenario involving commodity IoT devices. Results confirmed the utility and social acceptance of the ParaGlassMenu. Based on the results, we discuss implications for designing attention-maintaining subtle interaction techniques on OHMDs.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {721},
numpages = {21},
keywords = {social interaction, circular menu, conversation, Internet-of-Things, IoT manipulation, smart glasses, ring interaction, HMD},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581055,
author = {Li, Zhuojun and Yu, Chun and Gu, Yizheng and Shi, Yuanchun},
title = {ResType: Invisible and Adaptive Tablet Keyboard Leveraging Resting Fingers},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581055},
doi = {10.1145/3544548.3581055},
abstract = {Text entry on tablet touchscreens is a basic need nowadays. Tablet keyboards require visual attention for users to locate keys, thus not supporting efficient touch typing. They also take up a large proportion of screen space, which affects the access to information. To solve these problems, we propose ResType, an adaptive and invisible keyboard on three-state touch surfaces (e.g. tablets with unintentional touch prevention). ResType allows users to rest their hands on it and automatically adapts the keyboard to the resting fingers. Thus, users do not need visual attention to locate keys, which supports touch typing. We quantitatively explored users’ resting finger patterns on ResType, based on which we proposed an augmented Bayesian decoding algorithm for ResType, with 96.3% top-1 and 99.0% top-3 accuracies. After a 5-day evaluation, ResType achieved 41.26 WPM, outperforming normal tablet keyboards by 13.5% and reaching 86.7% of physical keyboards. It solves the occlusion problem while maintaining comparable typing speed with current methods on visible tablet keyboards.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {722},
numpages = {14},
keywords = {invisible keyboard, touch typing, adaptive keyboard, text entry},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580915,
author = {Faleel, Shariff AM and Liu, Yishuo and Cody, Roya A and Rey, Bradley and Du, Linghao and Yu, Jiangyue and Huang, Da-Yuan and Irani, Pourang and Li, Wei},
title = {T-Force: Exploring the Use of Typing Force for Three State Virtual Keyboards},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580915},
doi = {10.1145/3544548.3580915},
abstract = {Three state virtual keyboards which differentiate contact events between released, touched, and pressed states have the potential to improve overall typing experience and reduce the gap between virtual keyboards and physical keyboards. Incorporating force sensitivity, three-state virtual keyboards can utilize a force threshold to better classify a contact event. However, our limited knowledge of how force plays a role during typing on virtual keyboards limits further progress. Through a series of studies we observe that using a uniform threshold is not an optimal approach. Furthermore, the force being applied while typing varies significantly across the keys and among participants. As such, we propose three different approaches to further improve the uniform threshold. We show that a carefully selected non-uniform threshold function could be sufficient in delineating typing events on a three-state keyboard. Finally, we conclude our work with lessons learned, suggestion for future improvements, and comparisons with current methods available.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {723},
numpages = {15},
keywords = {three-state Virtual keyboard, force sensitive touch interactions, 10-finger typing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581248,
author = {Iftikhar, Zainab and Ma, Yumeng and Huang, Jeff},
title = {“Together but Not Together”: Evaluating Typing Indicators for Interaction-Rich Communication},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581248},
doi = {10.1145/3544548.3581248},
abstract = {Messaging is a ubiquitous digital communication medium. It is also a minimal medium of communication because of its inability to convey immediate feedback, tone, facial expressions, hesitations, and pauses, or follow the train of the other person’s thoughts. This paper combines quantitative and qualitative approaches for analyzing richer forms of typing indicators in messaging interfaces, such as showing text as it is typed. By assessing users’ subjective workload and interpreting these findings in the context of users’ experiences, we found that more expressive typing indicators were perceived as “rich in communication”, as they helped people communicate more allowing for closer connections. These indicators also increased users’ perceived co-presence. In addition, our research suggests there may be benefits of designing customized typing indicators for relationship maintenance and task-based communication.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {724},
numpages = {12},
keywords = {social presence, computer-mediated communication, online interaction, typing indicators, media richness theory, texting},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581321,
author = {Tao, Ye and Wang, Shuhong and Ji, Junzhe and Cai, Linlin and Xia, Hongmei and Wang, Zhiqi and He, Jinghai and Fan, Yitao and Pan, Shengzhang and Xu, Jinghua and Yang, Cheng and Sun, Lingyun and Wang, Guanyun},
title = {4Doodle: 4D Printing Artifacts Without 3D Printers},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581321},
doi = {10.1145/3544548.3581321},
abstract = {4D printing encodes transformability over time, which empowers users to create artifacts by on-demand deformation. The creative process of 4D printing shape-changing artifacts can be challenging because of its discontinuous fabrication steps, such as digital designing, specific path planning, automatic printing and manual triggering. We hypothesize that switching from typical 4D printing reliant on 3D printers to a more “handcrafted” method can allow users to understand and continuously reflect upon the artifact and its transformability. Towards this vision, we introduce 4Doodle, a hybrid craft approach that integrates unique deformation controllability and five techniques for freehand 4D printing, using a 3D pen. To tackle the shape-changing challenges of uncertain hands-on fabrication, we develop a mixed reality system to help novices master the manual skills of 4D printing. We also demonstrate a series of 4D printed artifacts with fully human intervention. Finally, our user study shows that 4Doodle lowers the skill-acquisition barrier associated with handcrafting 4D printed artifacts, and it has great potential for creative production and spatial ability.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {731},
numpages = {16},
keywords = {Hybrid craft, 3D pens, 4D printing, Shape-changing behavior},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581408,
author = {Jeong, Yunwoo and Cho, Hyungjun and Kim, Taewan and Nam, Tek-Jin},
title = {AutomataStage: An AR-Mediated Creativity Support Tool for Hands-on Multidisciplinary Learning},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581408},
doi = {10.1145/3544548.3581408},
abstract = {The creativity support tools can enhance the hands-on multidisciplinary learning experience by drawing interest in the process of creating the outcome. We present AutomataStage, an AR-mediated creativity support tool for hands-on multidisciplinary learning. AutomataStage utilizes a video see-through interface to support the creation of Interactive Automata. The combination of building blocks and low-cost materials increases the expressiveness. The generative design method and one-to-one guide support the idea development process. It also provides a hardware see-through feature with which inside parts and circuits can be seen and an operational see-through feature that shows the operation in real-time. The visual programming method with a state transition diagram supports the iterative process during the creation process. A user study shows that AutomataStage enabled the students to create diverse Interactive Automata within 40-minute sessions. By creating Interactive Automata, the participants could learn the basic concepts of components. See-through features allowed active exploration with interest while integrating the components. We discuss the implications of hands-on tools with interactive and kinetic content beyond multidisciplinary learning.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {732},
numpages = {16},
keywords = {hands-on learning, creativity support tool, video see-through system, learning tool, Interactive Automata, Multidisciplinary learning, STEAM},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580748,
author = {Ko, Donghyeon and Kim, Yoonji and Zhu, Junyi and Wessely, Michael and Mueller, Stefanie},
title = {FlexBoard: A Flexible Breadboard for Interaction Prototyping on Curved and Deformable Surfaces},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580748},
doi = {10.1145/3544548.3580748},
abstract = {We present FlexBoard, an interaction prototyping platform that enables rapid prototyping with interactive components such as sensors, actuators and displays on curved and deformable objects. FlexBoard offers the rapid prototyping capabilities of traditional breadboards but is also flexible to conform to different shapes and materials. FlexBoard’s bendability is enabled by replacing the rigid body of a breadboard with a flexible living hinge that holds the metal strips from a traditional breadboard while maintaining the standard pin spacing. In addition, FlexBoards are also shape-customizable as they can be cut to a specific length and joined together to form larger prototyping areas. We discuss FlexBoard’s mechanical design and present a technical evaluation of its bendability, adhesion to curved and deformable surfaces, and holding force of electronic components. Finally, we show the usefulness of FlexBoard through 3 application scenarios with interactive textiles, curved tangible user interfaces, and VR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {733},
numpages = {13},
keywords = {breadboards, deformable user interfaces., electronic prototyping},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580691,
author = {Pourjafarian, Narjes and Mjaku, Fjolla and Koelle, Marion and Schmitz, Martin and Borchers, Jan and Steimle, J\"{u}rgen},
title = {Handheld Tools Unleashed: Mixed-Initiative Physical Sketching with a Robotic Printer},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580691},
doi = {10.1145/3544548.3580691},
abstract = {Personal fabrication has mostly focused on handheld tools as embodied extensions of the user, and machines like laser cutters and 3D printers automating parts of the process without intervention. Although interactive digital fabrication has been explored as a middle ground, existing systems have a fixed allocation of user intervention vs. machine autonomy, limiting flexibility, creativity, and improvisation. We explore a new class of devices that combine the desirable properties of a handheld tool and an autonomous fabrication robot, offering a continuum from manual and assisted to autonomous fabrication, with seamless mode transitions. We exemplify the concept of mixed-initiative physical sketching with a working robotic printer that can be handheld for free-hand sketching, can provide interactive assistance during sketching, or move about for computer-generated sketches. We present interaction techniques to seamlessly transition between modes, and sketching techniques benefitting from these transitions to, e.g., extend (upscale, repeat) or revisit (refine, color) sketches. Our evaluation with seven sketchers illustrates that RoboSketch successfully leverages each mode’s strengths, and that mixed-initiative physical sketching makes computer-supported sketching more flexible.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {734},
numpages = {14},
keywords = {Sketching, fabrication, mixed-initiative fabrication, prototyping, robotic printer, sketching interfaces},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581002,
author = {Feng, Shuyue and Yao, Cheng and Lin, Weijia and Yao, Jiayu and Zhang, Chao and Jia, Zhongyu and Liu, Lijuan and Bokola, Masulani and Chen, Hangyue and Ying, Fangtian and Wang, Guanyun},
title = {MechCircuit: Augmenting Laser-Cut Objects with Integrated Electronics, Mechanical Structures and Magnets},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581002},
doi = {10.1145/3544548.3581002},
abstract = {Laser cutting revolutionizes the creation of personal-fabricated prototypes. These objects can have transformable properties by adopting different materials and be interactive by integrating electronic circuits. However, circuits in laser-cut objects always have limited movements, which refrains laser cutting from achieving interactive prototypes with more complex movable functions like mechanisms. We propose MechCircuit, a design and fabrication pipeline for making mechanical-electronical objects with laser cutting. We leverage the neodymium magnet’s natures of magnetism and conductivity to integrate electronics and mechanical structure joints into prototypes. We conduct the evaluation to explore technological parameters and assess the practical feasibility of the fabrication pipeline. And we organized a user-observing workshop for non-expert users. Through the outcoming prototypes, the result demonstrates the feasibility of MechCircuit as a useful and inspiring prototyping method.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {735},
numpages = {15},
keywords = {Digital fabrication;, Movable electronics, Prototyping},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581434,
author = {Albaugh, Lea and Hudson, Scott E and Yao, Lining},
title = {Physically Situated Tools for Exploring a Grain Space in Computational Machine Knitting},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581434},
doi = {10.1145/3544548.3581434},
abstract = {We propose an approach to enabling exploratory creativity in digital fabrication through the use of grain spaces. In material processes, “grain” describes underlying physical properties like the orientation of cellulose fibers in wood that, in aggregate, affect fabrication concerns (such as directional cutting) and outcomes (such as axes of strength and visual effects). Extending this into the realm of computational fabrication, grain spaces define a curated set of mid-level material properties as well as the underlying low-level fabrication processes needed to produce them. We specify a grain space for computational brioche knitting, use it to guide our production of a set of hybrid digital/physical tools to support quick and playful exploration of this space’s unique design affordances, and reflect on the role of such tools in creative practice.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {736},
numpages = {14},
keywords = {Casual Creativity., Machine Knitting, Textiles},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580847,
author = {Chang, Joseph Chee and Zhang, Amy X. and Bragg, Jonathan and Head, Andrew and Lo, Kyle and Downey, Doug and Weld, Daniel S.},
title = {CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580847},
doi = {10.1145/3544548.3580847},
abstract = {When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user’s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {737},
numpages = {15},
keywords = {reading interfaces, scientific papers, personalization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581371,
author = {Kang, Hyeonsu B and Soliman, Nouran and Latzke, Matt and Chang, Joseph Chee and Bragg, Jonathan},
title = {ComLittee: Literature Discovery with Personal Elected Author Committees},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581371},
doi = {10.1145/3544548.3581371},
abstract = {In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users’ evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee’s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors’ authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {738},
numpages = {20},
keywords = {author-augmented literature discovery, interpretable relevance explanations, Scholarly discovery systems, interactive machine learning, paper and author recommendations},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580741,
author = {Song, Da and Wang, Zhijie and Huang, Yuheng and Ma, Lei and Zhang, Tianyi},
title = {DeepLens: Interactive Out-of-Distribution Data Detection in NLP Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580741},
doi = {10.1145/3544548.3580741},
abstract = {Machine Learning (ML) has been widely used in Natural Language Processing (NLP) applications. A fundamental assumption in ML is that training data and real-world data should follow a similar distribution. However, a deployed ML model may suffer from out-of-distribution (OOD) issues due to distribution shifts in the real-world data. Though many algorithms have been proposed to detect OOD data from text corpora, there is still a lack of interactive tool support for ML developers. In this work, we propose DeepLens, an interactive system that helps users detect and explore OOD issues in massive text corpora. Users can efficiently explore different OOD types in DeepLens with the help of a text clustering method. Users can also dig into a specific text by inspecting salient words highlighted through neuron activation analysis. In a within-subjects user study with 24 participants, participants using DeepLens were able to find nearly twice more types of OOD issues accurately with 22% more confidence compared with a variant of DeepLens that has no interaction or visualization support.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {739},
numpages = {17},
keywords = {NLP, Out-of-distribution Detection, Interactive Visualization, Machine Learning},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580852,
author = {Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
title = {DeepSeer: Interactive RNN Explanation and Debugging via State Abstraction},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580852},
doi = {10.1145/3544548.3580852},
abstract = {Recurrent Neural Networks (RNNs) have been widely used in Natural Language Processing (NLP) tasks given its superior performance on processing sequential data. However, it is challenging to interpret and debug RNNs due to the inherent complexity and the lack of transparency of RNNs. While many explainable AI (XAI) techniques have been proposed for RNNs, most of them only support local explanations rather than global explanations. In this paper, we present DeepSeer, an interactive system that provides both global and local explanations of RNN behavior in multiple tightly-coordinated views for model understanding and debugging. The core of DeepSeer is a state abstraction method that bundles semantically similar hidden states in an RNN model and abstracts the model as a finite state machine. Users can explore the global model behavior by inspecting text patterns associated with each state and the transitions between states. Users can also dive into individual predictions by inspecting the state trace and intermediate prediction results of a given input. A between-subjects user study with 28 participants shows that, compared with a popular XAI technique, LIME, participants using DeepSeer made a deeper and more comprehensive assessment of RNN model behavior, identified the root causes of incorrect predictions more accurately, and came up with more actionable plans to improve the model performance.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {740},
numpages = {20},
keywords = {Visualization, Explainable AI, Recurrent Neural Networks, Model Debugging},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581290,
author = {Lam, Michelle S. and Ma, Zixian and Li, Anne and Freitas, Izequiel and Wang, Dakuo and Landay, James A. and Bernstein, Michael S.},
title = {Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581290},
doi = {10.1145/3544548.3581290},
abstract = {Machine learning practitioners often end up tunneling on low-level technical details like model architectures and performance metrics. Could early model development instead focus on high-level questions of which factors a model ought to pay attention to? Inspired by the practice of sketching in design, which distills ideas to their minimal representation, we introduce model sketching: a technical framework for iteratively and rapidly authoring functional approximations of a machine learning model’s decision-making logic. Model sketching refocuses practitioner attention on composing high-level, human-understandable concepts that the model is expected to reason over (e.g., profanity, racism, or sarcasm in a content moderation task) using zero-shot concept instantiation. In an evaluation with 17 ML practitioners, model sketching reframed thinking from implementation to higher-level exploration, prompted iteration on a broader range of model designs, and helped identify gaps in the problem formulation—all in a fraction of the time ordinarily required to build a model.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {741},
numpages = {24},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580841,
author = {Palani, Srishti and Naik, Aakanksha and Downey, Doug and Zhang, Amy X. and Bragg, Jonathan and Chang, Joseph Chee},
title = {Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580841},
doi = {10.1145/3544548.3580841},
abstract = {Scholars who want to research a scientific topic must take time to read, extract meaning, and identify connections across many papers. As scientific literature grows, this becomes increasingly challenging. Meanwhile, authors summarize prior research in papers’ related work sections, though this is scoped to support a single paper. A formative study found that while reading multiple related work paragraphs helps overview a topic, it is hard to navigate overlapping and diverging references and research foci. In this work, we design a system, Relatedly, that scaffolds exploring and reading multiple related work paragraphs on a topic, with features including dynamic re-ranking and highlighting to spotlight unexplored dissimilar information, auto-generated descriptive paragraph headings, and low-lighting of redundant information. From a within-subjects user study (n=15), we found that scholars generate more coherent, insightful, and comprehensive topic outlines using Relatedly compared to a baseline paper list.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {742},
numpages = {20},
keywords = {Scientific Discovery, Exploratory Search, Sensemaking, Literature Review},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581483,
author = {Keurulainen, Antti and Westerlund, Isak Rafael and Keurulainen, Oskar and Howes, Andrew},
title = {Amortised Experimental Design and Parameter Estimation for User Models of Pointing},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581483},
doi = {10.1145/3544548.3581483},
abstract = {User models play an important role in interaction design, supporting automation of interaction design choices. In order to do so, model parameters must be estimated from user data. While very large amounts of user data are sometimes required, recent research has shown how experiments can be designed so as to gather data and infer parameters as efficiently as possible, thereby minimising the data requirement. In the current article, we investigate a variant of these methods that amortises the computational cost of designing experiments by training a policy for choosing experimental designs with simulated participants. Our solution learns which experiments provide the most useful data for parameter estimation by interacting with in-silico agents sampled from the model space thereby using synthetic data rather than vast amounts of human data. The approach is demonstrated for three progressively complex models of pointing.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {772},
numpages = {17},
keywords = {adaptive experiment design, computational rationality, active inference, parameter estimation, user models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581439,
author = {Moon, Hee-Seung and Oulasvirta, Antti and Lee, Byungjoo},
title = {Amortized Inference with User Simulations},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581439},
doi = {10.1145/3544548.3581439},
abstract = {There have been significant advances in simulation models predicting human behavior across various interactive tasks. One issue remains, however: identifying the parameter values that best describe an individual user. These parameters often express personal cognitive and physiological characteristics, and inferring their exact values has significant effects on individual-level predictions. Still, the high complexity of simulation models usually causes parameter inference to consume prohibitively large amounts of time, as much as days per user. We investigated amortized inference for its potential to reduce inference time dramatically, to mere tens of milliseconds. Its principle is to pre-train a neural proxy model for probabilistic inference, using synthetic data simulated from a range of parameter combinations. From examining the efficiency and prediction performance of amortized inference in three challenging cases that involve real-world data (menu search, point-and-click, and touchscreen typing), the paper demonstrates that an amortized-inference approach permits analyzing large-scale datasets by means of simulation models. It also addresses emerging opportunities and challenges in applying amortized inference in HCI.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {773},
numpages = {20},
keywords = {inverse modeling, simulation models, amortized inference},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580686,
author = {Lim, Chungman and Park, Gunhyuk},
title = {Can a Computer Tell Differences between Vibrations?: Physiology-Based Computational Model for Perceptual Dissimilarity Prediction},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580686},
doi = {10.1145/3544548.3580686},
abstract = {Perceptual dissimilarities, requiring high-cost user ratings, have contributed to designing well-distinguishable vibrations for associated meaning delivery. Appropriate metrics can reduce the cost, but known metrics in vibration similarity/dissimilarity could not predict them robustly. We propose a physiology-based model (PM) that predicts the perceptual dissimilarities of a given vibration set via two parallel processes: Neural Coding (NC), mimicking the neural signal transfer, and One-dimensional Convolution (OC), capturing rhythmic features. Eight parameters were trained using six datasets published in the literature to maximize Spearman’s Rank Correlation. We validated PM and six metrics of RMSE, DTW, Spectral/Temporal Matchings, ST-SIM, and SPQI in twelve datasets: six trained and six untrained datasets including measured accelerations. In all validations, PM’s predictions showed robust correlations with user data and similar structures in perceptual spaces. Other baseline metrics showed better fit in specific datasets, but none of them robustly showed correlations and similar perceptual spaces over twelve datasets.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {774},
numpages = {15},
keywords = {Perceptual Dissimilarity, Vibrotactile Perception, Computational Model, Biomimetic Modeling},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581482,
author = {Suresh, Harini and Shanmugam, Divya and Chen, Tiffany and Bryan, Annie G and D'Amour, Alexander and Guttag, John and Satyanarayan, Arvind},
title = {Kaleidoscope: Semantically-Grounded, Context-Specific ML Model Evaluation},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581482},
doi = {10.1145/3544548.3581482},
abstract = {Desired model behavior often differs across contexts (e.g., different geographies, communities, or institutions), but there is little infrastructure to facilitate context-specific evaluations key to deployment decisions and building trust. Here, we present Kaleidoscope, a system for evaluating models in terms of user-driven, domain-relevant concepts. Kaleidoscope’s iterative workflow enables generalizing from a few examples into a larger, diverse set representing an important concept. These example sets can be used to test model outputs or shifts in model behavior in semantically-meaningful ways. For instance, we might construct a “xenophobic comments” set and test that its examples are more likely to be flagged by a content moderation model than a “civil discussion” set. To evaluate Kaleidoscope, we compare it against template- and DSL-based grouping methods, and conduct a usability study with 13 Reddit users testing a content moderation model. We find that Kaleidoscope facilitates iterative, exploratory hypothesis testing across diverse, conceptually-meaningful example sets.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {775},
numpages = {13},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580767,
author = {Gonzalez, Eric J and Follmer, Sean},
title = {Sensorimotor Simulation of Redirected Reaching Using Stochastic Optimal Feedback Control},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580767},
doi = {10.1145/3544548.3580767},
abstract = {Illusory VR interaction techniques such as hand redirection work because humans use vision to adjust their motor commands during movement (e.g., reaching). Existing simulations of redirected reaching are limited, however, and have not yet incorporated important stochastic characteristics like sensorimotor noise, nor captured redirection’s effect on movement duration. In this work, we propose adapting a stochastic optimal feedback control (SOFC) model of normal reach to simulate redirection by augmenting sensory feedback at run-time. We present a summary of our simulation and validate it against user data gathered in multiple redirection conditions. We also evaluate the impacts of visual attention on the effectiveness of redirection in real users and replicate the effects in simulation. Our results show that an infinite-horizon SOFC model is able to reproduce key characteristics of redirected reaches and highlight the benefits of SOFC as a tool for simulating, evaluating, and gaining insights about redirection techniques.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {776},
numpages = {17},
keywords = {Sensorimotor Control, Optimal Control, Stochastic Simulation, Hand Redirection, Virtual Reality, Modeling},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581217,
author = {Zhang, Hao and Huang, Jin and Tu, Huawei and Tian, Feng},
title = {Shape-Adaptive Ternary-Gaussian Model: Modeling Pointing Uncertainty for Moving Targets of Arbitrary Shapes},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581217},
doi = {10.1145/3544548.3581217},
abstract = {This paper presents a Shape-Adaptive Ternary-Gaussian model for describing endpoint uncertainty when pointing at moving targets of arbitrary shapes. The basic idea of the model is to combine the uncertainty related to the target shape with the uncertainty caused by the target motion. First, we proposed a model to predict endpoint distribution on static targets based on a Dual-Space Decomposition (DUDE) algorithm. Then, we linearly combined a 2D Ternary-Gaussian model with the newly proposed DUDE-based model to make the 2D Ternary-Gaussian model adaptable to moving targets with random shapes. To verify the performance of our model, we compared it with the original 2D Ternary-Gaussian model and a recent proposed Inscribed Circle model in predicting endpoint distribution. The results show that the proposed model outperformed the two baseline models while maintaining good robustness across different shapes and moving speeds.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {777},
numpages = {18},
keywords = {Pointing Uncertainty, Moving Target Selection, Models, Endpoint Distribution, Arbitrary Shapes},
location = {Hamburg, Germany},
series = {CHI '23}
}

